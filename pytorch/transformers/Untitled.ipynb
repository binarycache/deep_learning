{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b376c871",
   "metadata": {},
   "source": [
    "# Self-Attention and Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dab085",
   "metadata": {},
   "source": [
    "We have input in the form of word embeddings $ x_{i}, x_{j},....x_{n} $\n",
    "\n",
    "We want the output $y_{i}$ such that it tells us on which $x$ it focused.\n",
    "\n",
    "For ex: The animal didn't cross the street because it was too tired.\n",
    "\n",
    "In Language Modelling, when the NN is generating the word \"it\" what did the NN referred to, the \"animal\" or the \"street\".\n",
    "\n",
    "So we want to know the \"attention\" the NN paid to each of the word in the sentence while generating the word \"it\".\n",
    "\n",
    "In its simplest form this attention can be formaulated as dot product since it captures the similarity between two vectors.\n",
    "\n",
    "And this similarity could work as the weight for each of the word $j$ when the NN generated the word $i$\n",
    "\n",
    "So at any point of time $i$ the output $y_{i}$ can be given by $$ y_{i} = \\sum_{j} w_{ij} x_{i} $$\n",
    "\n",
    "where $$ w_{ij} = x_{i}^T x{j} $$ a.k.a the dot product with every other vector\n",
    "\n",
    "Now we can simply apply softmax to keep the values as probabilities.\n",
    "\n",
    "\n",
    "Let's see how we can implement this in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7832f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(x):\n",
    "    \"\"\"Function to apply self attention to the vector x.\n",
    "    Arguments\n",
    "    -------------------------\n",
    "    Shape of x: batch_size, t, k where t is the no. of vectors \n",
    "                and k is the size of each vector\n",
    "                i.e. x contains all the word embedding vectors\n",
    "\n",
    "    Returns:\n",
    "    --------------------------\n",
    "\n",
    "\n",
    "    Notes:\n",
    "    --------------------------\n",
    "    Assume we have a tensor of size (batch_size, t vectors, k (dimension of each vector))\n",
    "    k is fixed as the size of the embedding layer and the encoding layer is fixed\n",
    "    so for each vector we have the same size.\n",
    "    \"\"\"\n",
    "    # torch.bmm is batched matrix multiplication\n",
    "    # this is basically xx' for calculating weights wij.\n",
    "    raw_weights = torch.bmm(x, x.transpose(1,2))\n",
    "\n",
    "    # Turning these weights into probabilites by applying row-wise softmax\n",
    "    weights = F.softmax(raw_weights, dim=2)\n",
    "\n",
    "    # y = wx to get the output vector of size (b, t, k)\n",
    "    y = torch.bmm(weights,x)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ab08b",
   "metadata": {},
   "source": [
    "## query, keys and values?\n",
    "\n",
    "In the self-attention stage, every input vector is used in three ways.</br>\n",
    "    1. Compared to every other vector to establish the weights of its own output. (Query) </br>\n",
    "    2. Compared to every other vector to establish the weights for output of jth vector Yj. (Key)</br>\n",
    "    3. It is used as part of the weighted sum to compute each output vector \n",
    "       once the weights are established. (Value)</br>\n",
    "       **Need more clearance on the use of Values**\n",
    "    \n",
    "To get to these vectors or to make them play these roles, we introduce new parameters in the form of three matrices.\n",
    "\n",
    "In other words we add three $k x k$ (k is embedding dimension) matrices called $W_{q}$, $W_{k}$, $W_{v}$ and calcualte three *linear transformations* of the input vector $x{i}$\n",
    "\n",
    "$$ q_{i} = W_q x_i  ~~~~~ k_i = W_k x_i ~~~~~~~~~ v_i = W_v x_i $$\n",
    "\n",
    "Again, $q_i$ is weights of its **own** output, $k_i$ is with other\n",
    "\n",
    "$$ w_{ij}' = q_{i}' k_{j} $$\n",
    "\n",
    "$$ w_{ij} = softmax(w_{ij}') $$\n",
    "\n",
    "$$ y_i = \\sum_{j} W_{ij}v_j$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac2801",
   "metadata": {},
   "source": [
    "## Scaling the dot product\n",
    "\n",
    "Softmax is sensitive to very large values and the  average value of the dot product increases as the embedding dimension $k$ increases which kills the gradients and stops training. \n",
    "\n",
    "Solution: Scale the dot product\n",
    "\n",
    "How? \n",
    "\n",
    "\\begin{equation}\n",
    "  w_{ij}' = \\frac{q_{i}^T k_{j}}{\\sqrt{k}}\n",
    "\\end{equation}\n",
    "\n",
    "why $\\sqrt{k}$? Imagine a vector in $‚Ñù^k$ with values all c. Its Euclidean length is k$\\sqrt{c}$. Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665cb068",
   "metadata": {},
   "source": [
    "## Let's move to multi-head attention\n",
    "\n",
    "A word can mean different things to different neighbors.\n",
    "\n",
    "Ex: Mary gave roses to susan. The word \"gave\" contains different meanings for Mary and roses and Susan. Mary is the giver and susan is the receiver. With simple attention it will sum up all the weights together once and that's it.\n",
    "\n",
    "In multiple attention, we apply this attention multiple times for the same word so that the same word \"gave\" can apply different weight to Mary in different scenarios. These are called attention heads and are represented by \"r\".\n",
    "\n",
    "So the new matrices will be $W_{q}^r$, $W_{k}^r$, and $W_{v}^r$\n",
    "\n",
    "Now we pass an input $x_i$ through all these attention heads and finally **concatenate** them and pass them through a linear layer to reduce the dimension back to the embedding dimension $k$.\n",
    "\n",
    "Although if we simply make multiple copies of a single attention with their own query, key and value then it will be much too computationally expensive.\n",
    "\n",
    "A better way to do this is to cut the input vector into chunks of size equal to $k$/no_of_attention_heads.\n",
    "\n",
    "\n",
    "Let's code now!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc59a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.module):\n",
    "    '''\n",
    "    Implementation of a simple multi-head attention\n",
    "    \n",
    "    Note: This is not the optimized version, here we will have n_heads copies of \n",
    "    query, key and value.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, k:int, n_heads:int = 8):\n",
    "        '''\n",
    "        Args:\n",
    "        ---------------------------\n",
    "        k: Embedding dimension \n",
    "        n_heads: Number of multi-attention heads\n",
    "        \n",
    "        Notes:\n",
    "        ---------------------------\n",
    "        We think of the h attention heads as h separate sets of three matrices \n",
    "        ùêñ_q^r, ùêñ_k^r,ùêñ_v^r, but it's actually more efficient to combine these for \n",
    "        all heads into three single k√óhk matrices, \n",
    "        so that we can compute all the concatenated queries, keys and values \n",
    "        in a single matrix multiplication.\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.k = k \n",
    "        self.n_heads = n_heads\n",
    "        # These compute the queries, keys and values for all \n",
    "        # heads (as a single concatenated vector)\n",
    "        # this will generate the weight matrices Wq^r, Wk^r, Wv^r\n",
    "        self.tokeys    = nn.Linear(k, k * n_heads, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k * n_heads, bias=False)\n",
    "        self.tovalues  = nn.Linear(k, k * n_heads, bias=False)\n",
    "\n",
    "        # This unifies the outputs of the different heads into \n",
    "        # a single k-vector\n",
    "        self.unifyheads = nn.Linear(n_heads * k, k)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            '''\n",
    "            Shape of x: batch_size, t, k where t is the no. of vectors \n",
    "            and k is the size of each vector\n",
    "            i.e. x contains all the word embedding vectors\n",
    "            '''\n",
    "            b, t, k = x.size()\n",
    "            h = self.n_heads\n",
    "            # view is similar to numpy's reshape but it doesn't create any copies\n",
    "            # The output of each of the linear layers will be \n",
    "            # b, t, h*k which we are reshaping as (b,t,h,k)\n",
    "            queries = self.toqueries(x).view(b, t, h, k)\n",
    "            keys    = self.tokeys(x).view(b, t, h, k)\n",
    "            values  = self.tovalues(x) .view(b, t, h, k)\n",
    "            \n",
    "            # Next, we need to compute the dot products. This is the same operation\n",
    "            # for every head, so we fold the heads into the batch dimension. \n",
    "            # This ensures that we can use torch.bmm() as before, \n",
    "            # and the whole collection of keys, queries and values \n",
    "            # will just be seen as a slightly larger batch.\n",
    "            # Since the head and batch dimension are not next to each other,\n",
    "            # we need to transpose before we reshape. \n",
    "            # (This is costly, but it seems to be unavoidable.)\n",
    "            # contiguous means to make a copy of data so that the order of elements \n",
    "            # remains unchanged inspite of using view.\n",
    "            queries = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "            keys = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "            values = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "            \n",
    "            # let's scale these before doing dot product.\n",
    "            # instead scale the keys and queries by fourth root of k before multiplying them together. \n",
    "            # This should save memory for longer sequences.\n",
    "            queries = queries / (k ** (1/4))\n",
    "            keys    = keys / (k ** (1/4))\n",
    "            \n",
    "            \n",
    "            # - get dot product of scaled queries and keys\n",
    "            dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "            # - dot has size (b*h, t, t) containing raw weights\n",
    "\n",
    "            dot = F.softmax(dot, dim=2) \n",
    "            # - dot now contains row-wise normalized weights\n",
    "            \n",
    "            # apply the self attention to the values\n",
    "            out = torch.bmm(dot, values).view(b, h, t, k)\n",
    "            \n",
    "            # swap h, t back, unify heads\n",
    "            out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
    "            return self.unifyheads(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
