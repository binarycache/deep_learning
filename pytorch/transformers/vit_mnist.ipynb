{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d53fe96",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=ovB0ddFtzzA\n",
    "\n",
    "https://github.com/rwightman/pytorch-image-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e14192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1d03e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(torch.nn.Module):\n",
    "    '''Split image into patches and then embeds them\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    img_size:int Size of the image (it's a square)\n",
    "    \n",
    "    patch_size:int Size of the patch (it's a square)\n",
    "    \n",
    "    in_channels:int Number of input channels\n",
    "    \n",
    "    embed_dimensions:int The embedding dimension\n",
    "    \n",
    "    Attributes\n",
    "    ---------------\n",
    "    n_patches:int Number of patches inside our image\n",
    "    \n",
    "    proj:nn.Conv2D Convolution layer to do patching and their embedding both\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, img_size:int, patch_size:int, \n",
    "                 in_channels:int = 3, embed_dimensions:int = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dimension = embed_dimensions\n",
    "        self.n_patches = (img_size//patch_size)**2\n",
    "        \n",
    "        # Both kernel and stride size is set to patch size to get\n",
    "        # non-overlapping patches\n",
    "        self.proj = torch.nn.Conv2d(in_channels, embed_dimensions, \n",
    "                                    kernel_size=patch_size,\n",
    "                                    stride=patch_size\n",
    "                                    )\n",
    "    def forward(self,x):\n",
    "        '''Run forward pass\n",
    "        Parameters\n",
    "        ----------------\n",
    "        x:torch.Tensor Shape(n_samples/batch_size, in_channels, img_size, img_size)\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        torch.Tensor Shape(batch_size, n_patches, embed_dimension)\n",
    "        \n",
    "        '''\n",
    "        x = self.proj(x) # batch_size x embed_dim x n_patches**0.5 x n_patches**0.5\n",
    "        x = x.flatten(2) # batch_size x embed_dim x n_patches\n",
    "        x = x.transpose(1,2) # batch_size x n_patches x embed_dim\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a257e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    '''Multi-headed Self Attention Mechanism\n",
    "    \n",
    "    Parameters:\n",
    "    ---------------\n",
    "    dim: int \n",
    "        Input and Output dimension of per token features.\n",
    "    \n",
    "    n_heads: int\n",
    "        Number of Attention Heads\n",
    "    \n",
    "    qkv_bias: bool\n",
    "        If True, include bias in query, key and values projections.\n",
    "    \n",
    "    attn_p: float\n",
    "        Dropout probability applied to QKV tensors.\n",
    "    \n",
    "    proj_p: float\n",
    "        Dropout probability applied to the ouput tensor.\n",
    "        \n",
    "    Attributes:\n",
    "    -----------------\n",
    "    scale: float\n",
    "        Normalizing constant for the dot product\n",
    "    qkv: nn.Linear\n",
    "        Linear projection for the query, key and value \n",
    "    proj: torch.nn.Linear\n",
    "        Linear mapping that takes in the concactenated output of all attention heads\n",
    "        and maps it on to a new space.\n",
    "    attn_dropout, proj_dropout: torch.nn.Dropout\n",
    "        Dropout Layers\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,dim,n_heads=12,qkv_bias=True,attn_p=0.0,proj_p=0.0):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        # This would be the dimension of the concatenated attention heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5 # (1/sqrt(k))\n",
    "        \n",
    "        # This can also be broken into 3 separate linear layers\n",
    "        self.qkv = torch.nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.attn_drop = torch.nn.Dropout(attn_p)\n",
    "        self.proj = torch.nn.Linear(dim, dim)\n",
    "        self.proj_drop = torch.nn.Dropout(proj_p)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        '''Run forward pass\n",
    "        Parameters\n",
    "        ----------------\n",
    "        x: torch.Tensor \n",
    "            Shape(n_samples/batch_size, n_patches + 1, dim)\n",
    "            The + 1 comes from the class token\n",
    "            Note that input and output shape is the same.\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        torch.Tensor \n",
    "            Shape(batch_size, n_patches + 1, dim)\n",
    "        \n",
    "        '''\n",
    "        batch_size, n_tokens, dim = x.shape\n",
    "        \n",
    "        if dim!=self.dim:\n",
    "            raise ValueError\n",
    "        \n",
    "        # Usually the Linear layer expects 2D input of shape (batch_size, input_dimension)\n",
    "        # but if you give more dimensions than the last dimension should be input\n",
    "        qkv = self.qkv(x) # (batch_size, n_patches+1, 3*dim)\n",
    "        \n",
    "        # (batch_size, n_patches+1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.reshape(batch_size, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # reshape to simulate 3 different layers\n",
    "        qkv = qkv.permute(2,0,3,1,4) # (3,batch_size,n_heads,n_patches+1,head_dim)\n",
    "        \n",
    "        q, k , v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # transpose the keys to do dot product later\n",
    "        k_t = k.transpose(-2,-1)\n",
    "        \n",
    "        dp = (q @ k_t) * self.scale # (batch_size, n_heads, n_patches+1, n_patches+1)\n",
    "        \n",
    "        attn = dp.softmax(dim=-1) # (batch_size, n_heads, n_patches+1, n_patches+1)\n",
    "        \n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        weighted_avg = attn @ v # (batch_size, n_heads, n_patches+1, head_dim)\n",
    "        \n",
    "        weighted_avg = weighted_avg.transpose(1,2) # (batch_size, n_patches+1,n_heads, head_dim)\n",
    "        \n",
    "        weighted_avg = weighted_avg.flatten(2) # (batch_size, n_patches+1, dim)\n",
    "        \n",
    "        x = self.proj(weighted_avg) # (batch_size, n_patches+1, dim)\n",
    "        x = self.proj_drop(x) # (batch_size, n_patches+1, dim)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1ee244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    '''Multi-Layer Perceptron\n",
    "    Parameters\n",
    "    --------------\n",
    "    in_features: int\n",
    "        Number of input features\n",
    "    hidden_features: int\n",
    "        Number of nodes in the hidden layer\n",
    "    out_features: int\n",
    "        Number of output features\n",
    "    p: float\n",
    "        Dropout Probability\n",
    "    Attributes\n",
    "    ---------------\n",
    "    fc1: nn.Linear\n",
    "        First linear layer\n",
    "    act: nn.GELU\n",
    "        Gaussian Error Linear Unit activation function\n",
    "    fc2: nn.Linear\n",
    "        Second Linear Layer\n",
    "    drop: nn.Dropput\n",
    "        Dropout Layer\n",
    "    '''\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = torch.nn.Linear(hidden_features, out_features)\n",
    "        self.drop = torch.nn.Dropout(p)\n",
    "        self.act = torch.nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Runs a forward pass\n",
    "        Parameter\n",
    "        -------------\n",
    "        x: torch.Tensor\n",
    "            (batch_size, n_patches + 1, in_features)\n",
    "            \n",
    "        Returns\n",
    "        --------------\n",
    "        torch.Tensor\n",
    "            (batch_size, n_patches+1, out_features)\n",
    "        '''\n",
    "        x = self.fc1(x) # (batch_size, n_patches + 1, hidden_features)\n",
    "        x = self.act(x) # (batch_size, n_patches + 1, hidden_features)\n",
    "        x = self.drop(x) # (batch_size, n_patches + 1, hidden_features)\n",
    "        x = self.fc2(x) # (batch_size, n_patches + 1, hidden_features)\n",
    "        x = self.drop(x) # (batch_size, n_patches + 1, hidden_features)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d91530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    '''Transformer Block\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    dim: int\n",
    "        Embedding dimension\n",
    "    n_heads: int\n",
    "        No. of self-attention heads\n",
    "    mlp_ratio: float\n",
    "        Determines the hidden dimension of the 'MLP' module w.r.t 'dim'\n",
    "    qkv_bias: bool\n",
    "        If True, include bias in query, key and values projections.\n",
    "    \n",
    "    p, attn_p: float\n",
    "        Dropout probability.\n",
    "    \n",
    "    Attributes:\n",
    "    norm1, norm2: LayerNorm\n",
    "        Layer Normalization\n",
    "    attn: SelfAttention \n",
    "        Self Attention Module\n",
    "    mlp: MLP\n",
    "        MLP Module\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, dim,n_heads,mlp_ratio=4.0,qkv_bias=True,p=0.,attn_p=0.):\n",
    "        super().__init__()\n",
    "        # You can set elementwise_affine=False to not train any parameters of LayerNorm\n",
    "        # default is True and it takes the mean and std. deviation for each sample/example\n",
    "        # therefore, it is independent of the batch_size unlike BatchNorm\n",
    "        self.norm1 = torch.nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = SelfAttention(\n",
    "                    dim,\n",
    "                    n_heads=n_heads,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    attn_p=attn_p,\n",
    "                    proj_p=p\n",
    "                    )\n",
    "        self.norm2 = torch.nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim*mlp_ratio)\n",
    "        self.MLP = MLP(\n",
    "                    in_features=dim,\n",
    "                    hidden_features=hidden_features,\n",
    "                    out_features=dim\n",
    "                    )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''Run forward pass\n",
    "        Parameters\n",
    "        ----------------\n",
    "        x: torch.Tensor \n",
    "            Shape(n_samples/batch_size, n_patches + 1, dim)\n",
    "            The + 1 comes from the class token\n",
    "            Note that input and output shape is the same.\n",
    "\n",
    "        Returns\n",
    "        ----------------\n",
    "        torch.Tensor \n",
    "            Shape(batch_size, n_patches + 1, dim)\n",
    "        '''\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.MLP(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(torch.nn.Module):\n",
    "    '''Simplified Implementation of Vision Transformers\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    img_size: int \n",
    "        Size of the image (it's a square)\n",
    "    \n",
    "    patch_size: int \n",
    "        Size of the patch (it's a square)\n",
    "    \n",
    "    in_channels: int \n",
    "        Number of input channels\n",
    "    \n",
    "    n_classes: int\n",
    "        Number of classes in the dataset\n",
    "    \n",
    "    embed_dimensions: int \n",
    "        The embedding dimension\n",
    "        \n",
    "    depth: int\n",
    "        Number of Transformer Blocks\n",
    "        \n",
    "    n_heads: int\n",
    "        No. of self-attention heads\n",
    "    mlp_ratio: float\n",
    "        Determines the hidden dimension of the 'MLP' module w.r.t 'dim'\n",
    "    qkv_bias: bool\n",
    "        If True, include bias in query, key and values projections.\n",
    "    \n",
    "    p, attn_p: float\n",
    "        Dropout probability.\n",
    "    \n",
    "    Attributes:\n",
    "    -------------\n",
    "    \n",
    "    patchEmbed: PatchEmbed\n",
    "        Instance of 'PatchEmbed' Layer\n",
    "        \n",
    "    clsToken: torch.nn.Parameter\n",
    "        Learnable Parameter of embed_dim dimensions. Representes first token in input.\n",
    "        \n",
    "    pos_embed: torch.nn.Parameter\n",
    "        Positional Encoding of all the patches + cls token\n",
    "        It has (n_patches+1)*embed_dim elements\n",
    "        \n",
    "    pos_dropout: torch.nn.Dropout\n",
    "        Dropout Layer\n",
    "    \n",
    "    transformer_blocks: torch.nn.ModuleList\n",
    "        List of TransformerBlock Modules\n",
    "    \n",
    "    norm: torch.nn.LayerNorm\n",
    "        Layer Normalization\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def __init__(self,img_size=384,\n",
    "                 patch_size=16,\n",
    "                 in_channels=3,\n",
    "                 n_classes=1000,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 n_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 qkv_bias=True,\n",
    "                 attn_p=0.0,\n",
    "                 p=0.\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(\n",
    "                            img_size=img_size,\n",
    "                            patch_size=patch_size,\n",
    "                            in_channels=in_channels,\n",
    "                            embed_dim=embed_dim\n",
    "                            )\n",
    "        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = torch.nn.Parameter(\n",
    "                                torch.zeros(1,1+self.patch_embed.n_patches,embed_dim))\n",
    "        self.pos_drop = torch.nn.Dropout(p=p)\n",
    "        self.blocks = torch.nn.ModuleList(\n",
    "                    [\n",
    "                        TransformerBlock(\n",
    "                        dim=embed_dim,\n",
    "                        n_heads=n_heads,\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        p=p,\n",
    "                        attn_bias=attn_bias) \n",
    "                        for _ in range(depth)\n",
    "                    ]\n",
    "                        )\n",
    "        self.norm = torch.nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = torch.nn.Linear(embed_dim,n_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            '''Run the forward pass\n",
    "            \n",
    "            Parameters:\n",
    "            ---------------\n",
    "            x: torch.Tensor\n",
    "                Shape (batch_size, in_channels, img_size, img_size)\n",
    "            Returns:\n",
    "            logits: torch.Tensor\n",
    "                Logits over all the classes (batch_size, n_classes)\n",
    "            \n",
    "            '''\n",
    "            batch_size = x.shape[0]\n",
    "            x = self.patch_embed(x)\n",
    "            cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "            x = torch.cat((cls_token,x),dim=-1) # (batch_size, n_patches+1, embed_dim)\n",
    "            x = x + self.pos_embed() # (batch_size, n_patches+1, embed_dim)\n",
    "            x = self.pos_drop(x)\n",
    "            \n",
    "            for block in self.blocks:\n",
    "                x = block(x)\n",
    "            \n",
    "            x = self.norm(x)\n",
    "            cls_token_final = x[:,0] # only selecting the class token\n",
    "            x = self.head(cls_token_final)\n",
    "            return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
