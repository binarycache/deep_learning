{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba80121",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Simple Code: \n",
    "\n",
    "1. http://peterbloem.nl/blog/transformers\n",
    "2. https://github.com/feather-ai/transformers-tutorial\n",
    "3. https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Optimized Multi-GPU (For version 2):\n",
    "http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ff1d3",
   "metadata": {},
   "source": [
    "# Self-Attention and Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0866f60",
   "metadata": {},
   "source": [
    "We have input in the form of word embeddings $ x_{i}, x_{j},....x_{n} $\n",
    "\n",
    "We want the output $y_{i}$ such that it tells us on which $x$ it focused.\n",
    "\n",
    "For ex: The animal didn't cross the street because it was too tired.\n",
    "\n",
    "In Language Modelling, when the NN is generating the word \"it\" what did the NN referred to, the \"animal\" or the \"street\".\n",
    "\n",
    "So we want to know the \"attention\" the NN paid to each of the word in the sentence while generating the word \"it\".\n",
    "\n",
    "In its simplest form this attention can be formaulated as dot product since it captures the similarity between two vectors.\n",
    "\n",
    "And this similarity could work as the weight for each of the word $j$ when the NN generated the word $i$\n",
    "\n",
    "So at any point of time $i$ the output $y_{i}$ can be given by $$ y_{i} = \\sum_{j} w_{ij} x_{i} $$\n",
    "\n",
    "where $$ w_{ij} = x_{i}^T x{j} $$ a.k.a the dot product with every other vector\n",
    "\n",
    "Now we can simply apply softmax to keep the values as probabilities.\n",
    "\n",
    "\n",
    "Let's see how we can implement this in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a147c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(x):\n",
    "    \"\"\"Function to apply self attention to the vector x.\n",
    "    Arguments\n",
    "    -------------------------\n",
    "    Shape of x: batch_size, t, k where t is the no. of vectors \n",
    "                and k is the size of each vector\n",
    "                i.e. x contains all the word embedding vectors\n",
    "\n",
    "    Returns:\n",
    "    --------------------------\n",
    "\n",
    "\n",
    "    Notes:\n",
    "    --------------------------\n",
    "    Assume we have a tensor of size (batch_size, t vectors, k (dimension of each vector))\n",
    "    k is fixed as the size of the embedding layer and the encoding layer is fixed\n",
    "    so for each vector we have the same size.\n",
    "    \"\"\"\n",
    "    # torch.bmm is batched matrix multiplication\n",
    "    # this is basically xx' for calculating weights wij.\n",
    "    raw_weights = torch.bmm(x, x.transpose(1,2))\n",
    "\n",
    "    # Turning these weights into probabilites by applying row-wise softmax\n",
    "    weights = F.softmax(raw_weights, dim=2)\n",
    "\n",
    "    # y = wx to get the output vector of size (b, t, k)\n",
    "    y = torch.bmm(weights,x)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef71b9",
   "metadata": {},
   "source": [
    "## query, keys and values?\n",
    "\n",
    "In the self-attention stage, every input vector is used in three ways.</br>\n",
    "    1. Compared to every other vector to establish the weights of its own output. (Query) </br>\n",
    "    2. Compared to every other vector to establish the weights for output of jth vector Yj. (Key)</br>\n",
    "    3. It is used as part of the weighted sum to compute each output vector \n",
    "       once the weights are established. (Value)</br>\n",
    "       **Need more clearance on the use of Values**\n",
    "    \n",
    "To get to these vectors or to make them play these roles, we introduce new parameters in the form of three matrices.\n",
    "\n",
    "In other words we add three $k x k$ (k is embedding dimension) matrices called $W_{q}$, $W_{k}$, $W_{v}$ and calcualte three *linear transformations* of the input vector $x{i}$\n",
    "\n",
    "$$ q_{i} = W_q x_i  ~~~~~ k_i = W_k x_i ~~~~~~~~~ v_i = W_v x_i $$\n",
    "\n",
    "Again, $q_i$ is weights of its **own** output, $k_i$ is with other\n",
    "\n",
    "$$ w_{ij}' = q_{i}' k_{j} $$\n",
    "\n",
    "$$ w_{ij} = softmax(w_{ij}') $$\n",
    "\n",
    "$$ y_i = \\sum_{j} W_{ij}v_j$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd594bd",
   "metadata": {},
   "source": [
    "## Scaling the dot product\n",
    "\n",
    "Softmax is sensitive to very large values and the  average value of the dot product increases as the embedding dimension $k$ increases which kills the gradients and stops training. \n",
    "\n",
    "Solution: Scale the dot product\n",
    "\n",
    "How? \n",
    "\n",
    "\\begin{equation}\n",
    "  w_{ij}' = \\frac{q_{i}^T k_{j}}{\\sqrt{k}}\n",
    "\\end{equation}\n",
    "\n",
    "why $\\sqrt{k}$? Imagine a vector in $‚Ñù^k$ with values all c. Its Euclidean length is k$\\sqrt{c}$. Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180fbb72",
   "metadata": {},
   "source": [
    "## Let's move to multi-head attention\n",
    "\n",
    "A word can mean different things to different neighbors.\n",
    "\n",
    "Ex: Mary gave roses to susan. The word \"gave\" contains different meanings for Mary and roses and Susan. Mary is the giver and susan is the receiver. With simple attention it will sum up all the weights together once and that's it.\n",
    "\n",
    "In multiple attention, we apply this attention multiple times for the same word so that the same word \"gave\" can apply different weight to Mary in different scenarios. These are called attention heads and are represented by \"r\".\n",
    "\n",
    "So the new matrices will be $W_{q}^r$, $W_{k}^r$, and $W_{v}^r$\n",
    "\n",
    "Now we pass an input $x_i$ through all these attention heads and finally **concatenate** them and pass them through a linear layer to reduce the dimension back to the embedding dimension $k$.\n",
    "\n",
    "Although if we simply make multiple copies of a single attention with their own query, key and value then it will be much too computationally expensive.\n",
    "\n",
    "A better way to do this is to cut the input vector into chunks of size equal to $k$/no_of_attention_heads.\n",
    "\n",
    "\n",
    "Let's code now!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.module):\n",
    "    '''\n",
    "    Implementation of a simple multi-head attention\n",
    "    \n",
    "    Note: This is not the optimized version, here we will have n_heads copies of \n",
    "    query, key and value.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, k:int, n_heads:int = 8):\n",
    "        '''\n",
    "        Args:\n",
    "        ---------------------------\n",
    "        k: Embedding dimension \n",
    "        n_heads: Number of multi-attention heads\n",
    "        \n",
    "        Notes:\n",
    "        ---------------------------\n",
    "        We think of the h attention heads as h separate sets of three matrices \n",
    "        ùêñ_q^r, ùêñ_k^r,ùêñ_v^r, but it's actually more efficient to combine these for \n",
    "        all heads into three single k√óhk matrices, \n",
    "        so that we can compute all the concatenated queries, keys and values \n",
    "        in a single matrix multiplication.\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.k = k \n",
    "        self.n_heads = n_heads\n",
    "        # These compute the queries, keys and values for all \n",
    "        # heads (as a single concatenated vector)\n",
    "        # this will generate the weight matrices Wq^r, Wk^r, Wv^r\n",
    "        self.tokeys    = nn.Linear(k, k * n_heads, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k * n_heads, bias=False)\n",
    "        self.tovalues  = nn.Linear(k, k * n_heads, bias=False)\n",
    "\n",
    "        # This unifies the outputs of the different heads into \n",
    "        # a single k-vector\n",
    "        self.unifyheads = nn.Linear(n_heads * k, k)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            '''\n",
    "            Shape of x: batch_size, t, k where t is the no. of vectors \n",
    "            and k is the size of each vector\n",
    "            i.e. x contains all the word embedding vectors\n",
    "            '''\n",
    "            b, t, k = x.size()\n",
    "            h = self.n_heads\n",
    "            # view is similar to numpy's reshape but it doesn't create any copies\n",
    "            # The output of each of the linear layers will be \n",
    "            # b, t, h*k which we are reshaping as (b,t,h,k)\n",
    "            queries = self.toqueries(x).view(b, t, h, k)\n",
    "            keys    = self.tokeys(x).view(b, t, h, k)\n",
    "            values  = self.tovalues(x) .view(b, t, h, k)\n",
    "            \n",
    "            # Next, we need to compute the dot products. This is the same operation\n",
    "            # for every head, so we fold the heads into the batch dimension. \n",
    "            # This ensures that we can use torch.bmm() as before, \n",
    "            # and the whole collection of keys, queries and values \n",
    "            # will just be seen as a slightly larger batch.\n",
    "            # Since the head and batch dimension are not next to each other,\n",
    "            # we need to transpose before we reshape. \n",
    "            # (This is costly, but it seems to be unavoidable.)\n",
    "            # contiguous means to make a copy of data so that the order of elements \n",
    "            # remains unchanged inspite of using view.\n",
    "            queries = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "            keys = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "            values = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "            \n",
    "            # let's scale these before doing dot product.\n",
    "            # instead scale the keys and queries by fourth root of k before multiplying them together. \n",
    "            # This should save memory for longer sequences.\n",
    "            queries = queries / (k ** (1/4))\n",
    "            keys    = keys / (k ** (1/4))\n",
    "            \n",
    "            \n",
    "            # - get dot product of scaled queries and keys\n",
    "            dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "            # - dot has size (b*h, t, t) containing raw weights\n",
    "\n",
    "            dot = F.softmax(dot, dim=2) \n",
    "            # - dot now contains row-wise normalized weights\n",
    "            \n",
    "            # apply the self attention to the values\n",
    "            out = torch.bmm(dot, values).view(b, h, t, k)\n",
    "            \n",
    "            # swap h, t back, unify heads\n",
    "            out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
    "            return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f1585",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Any architecture designed to process a connected set of units‚Äîsuch \n",
    "as the tokens in a sequence or the pixels in an image‚Äîwhere the only \n",
    "interaction between units is through self-attention.\n",
    "\n",
    "![Transfomer Block](http://peterbloem.nl/files/transformers/transformer-block.svg)\n",
    "\n",
    "the block applies, in sequence: a self attention layer, layer normalization, a feed forward layer (a single MLP applied independently to each vector), and another layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807649e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(k, n_heads=heads)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(k)\n",
    "        self.norm2 = nn.LayerNorm(k)\n",
    "        \n",
    "        # We‚Äôve made the relatively arbitrary choice of making the hidden layer \n",
    "        # of the feedforward 4 times as big as the input and output. \n",
    "        # Smaller values may work as well, and save memory, \n",
    "        # but it should be bigger than the input/output layers.\n",
    "        self.ff = nn.Sequential(\n",
    "          nn.Linear(k, 4 * k),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4 * k, k))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "        fedforward = self.ff(x)\n",
    "        return self.norm2(fedforward + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791aec09",
   "metadata": {},
   "source": [
    "## Let's build a classifier using Transformers\n",
    "\n",
    "![Sentiment Classification](http://peterbloem.nl/files/transformers/classifier.svg)\n",
    "\n",
    "**position embeddings** </br>\n",
    "We simply embed the positions like we did the words. Just like we created embedding vectors ùêØcat and ùêØsusan, we create embedding vectors ùêØ12 and ùêØ25. Up to however long we expect sequences to get. The drawback is that we have to see sequences of every length during training, otherwise the relevant position embeddings don't get trained. The benefit is that it works pretty well, and it's easy to implement.</br>\n",
    "\n",
    "**position encodings**</br>\n",
    "Position encodings work in the same way as embeddings, except that we don't learn the position vectors, we just choose some function f:‚Ñï‚Üí‚Ñùk to map the positions to real valued vectors, and let the network figure out how to interpret these encodings. The benefit is that for a well chosen function, the network should be able to deal with sequences that are longer than those it's seen during training (it's unlikely to perform well on them, but at least we can check). The drawbacks are that the choice of encoding function is a complicated hyperparameter, and it complicates the implementation a little.\n",
    "\n",
    "\n",
    "For the sake of simplicity, we‚Äôll use position embeddings in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096fb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_emb = nn.Embedding(num_tokens, k)\n",
    "        self.pos_emb = nn.Embedding(seq_length, k)\n",
    "\n",
    "        # The sequence of transformer blocks that does all the \n",
    "        # heavy lifting\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(k=k, heads=heads))\n",
    "        \n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        # Maps the final output sequence to class logits\n",
    "        self.toprobs = nn.Linear(k, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A (b, t) tensor of integer values representing \n",
    "                  words (in some predetermined vocabulary).\n",
    "        :return: A (b, c) tensor of log-probabilities over the \n",
    "                 classes (where c is the nr. of classes).\n",
    "        \"\"\"\n",
    "        # generate token embeddings\n",
    "        tokens = self.token_emb(x)\n",
    "        b, t, k = tokens.size()\n",
    "\n",
    "        # generate position embeddings\n",
    "        positions = torch.arange(t)\n",
    "        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
    "        \n",
    "        x = tokens + positions\n",
    "        x = self.tblocks(x)\n",
    "        \n",
    "        # Average-pool over the t dimension and project to class \n",
    "        # probabilities\n",
    "        x = self.toprobs(x.mean(dim=1))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77c1dc",
   "metadata": {},
   "source": [
    "# What is Position Encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc9f7b",
   "metadata": {},
   "source": [
    "Intuition: https://www.youtube.com/watch?v=1biZfFLPRSY\n",
    "Code: https://www.youtube.com/watch?v=LSCsfeEELso\n",
    "\n",
    "We add the position information to the embeddings because unlike RNN there is no position (or time) in Transformers.\n",
    "\n",
    "The input to the NLP models is usually an embedding of the words. So the logical thing to do is to add this positional information in the embeddings itself instead of the actual input.\n",
    "\n",
    "Note: We are not simply adding the position of the word in the sentence, rather treat this as an embedding which adds semantic information to the vectors.\n",
    "\n",
    "The way the authors proposed this is by adding a vector to the embedding vector.\n",
    "\n",
    "So to add these embedding and posistional encoding (PE) vectors, we need same dimensions.\n",
    "\n",
    "Now consider the vector PE of dimension d = embedding dimension,\n",
    "\n",
    "the positions are encoded using Sinousuidals.\n",
    "\n",
    "$$ PE_{pos,2i} = sin(\\frac{pos}{10000^{2i/d}}) $$ and\n",
    "\n",
    "$$ PE_{pos,2i+1} = cos(\\frac{pos}{10000^{2i/d}}) $$\n",
    "\n",
    "where pos is the position of the word and i is the index of d-dimensional vector.\n",
    "\n",
    "Why sinosuidal? \n",
    "a. Its always between 0 and 1 so your positional encoding never empowers the embedding which contains semantic information.\n",
    "b. Its infinite so the sequence length never becomes an issue.\n",
    "c. In comparison to sigmoid, sin and cosine have veriability for big numbers as well, while the sigmoid becomes 1 after a fixed number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90e424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
